{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 8 Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "8.1. Introduction to Stream Processing with Spark\n",
    "  8.1.1. Spark Streaming API (DStream)\n",
    "  8.1.2. Structured Streaming API\n",
    "  8.1.3. Stream Processing Model\n",
    "  8.1.4. Input Sources\n",
    "  8.1.5. Output Sinks\n",
    "  8.1.6. Output Mode\n",
    "  8.1.7. Fault Tolerance and Restarts\n",
    "  \n",
    "8.2. Windowing and Aggregates\n",
    "  8.2.1. Stateless vs Stateful Transformations\n",
    "  8.2.2. Event Time and Windowing    \n",
    "  8.2.3. Tumbling Window\n",
    "  8.2.4. Sliding Window\n",
    "  8.2.5. Watermarking\n",
    "  \n",
    "8.3. Joins\n",
    "  8.3.1. Joining to a Static Source\n",
    "  8.3.2. Joining to Another Stream\n",
    "  8.3.3. Watermark\n",
    "  8.3.4. Outer Joins\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Stream Processing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start reviewing how Spark operates in the standard batch processing mode:\n",
    "![Standard batch processing operation](https://bigdata.cesga.es/img/spark_streaming-non_streaming_operation.png)\n",
    "In batch mode, we have a input data source, we apply some transformations and we write the output to the given storage.\n",
    "\n",
    "When procesing streaming data source we have to introduce a new axis, **time**, because in this case the input source is constantly generating new input data as time evolves.\n",
    "\n",
    "![Microbatches](https://bigdata.cesga.es/img/spark_streaming-microbatch_generation.png)\n",
    "In stream processing mode Spark divides the input data stream in micro-batches and then each micro-batch is processed in a series of small jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming API (DStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Streaming API, aka DStream, is the implementation of Spark Streaming based on RDDs. You can find it in legacy projects but for new projects the newer Structured Streaming API is recommended.\n",
    "\n",
    "NOTE: There are no longer updates to Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Structured Streaming API is the new streaming API that uses the Spark SQL engine, ie. the DataFrame API.\n",
    "\n",
    "Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "The idea behind both Spark Streaming and Structured Streaming is to divide the stream of data into **micro-batches** and each micro-batch its processed as a small job, achieving end-to-end latencies as low as 100 milliseconds.\n",
    "\n",
    "To achive lower latencies, there is also a low-latency processing mode called **Continuous Processing** which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees.\n",
    "\n",
    "The Spark SQL engine (Catalyst) takes care of running the series of jobs incrementally and continuously updating the final result as streaming data continues to arrive.\n",
    "\n",
    "The system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind spark structured streaming is to treat the live data stream as a table that is being continously appended.\n",
    "\n",
    "![Unbounded table](https://bigdata.cesga.es/img/spark_streaming-unbounded_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Socket source (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rate source (for testing and benchmarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the number of partitions to simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .option(\"numPartitions\", 2) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"json\") \\\n",
    "  .option(\"path\", \"path/to/source/dir\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be: parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab: Unit_8_structured_streaming-dataframe_schema.py\n",
    "- Review the code\n",
    "- Run the app using:\n",
    "```\n",
    "    spark-submit Unit_8_structured_streaming-dataframe_schema.py bigdata.cesga.es 80\n",
    "```    \n",
    "- What is the schema of the dataframe that is generated from the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab: Unit_8_input_source_rate.py\n",
    "- Check how the rate input source works. This source is very useful for testing.\n",
    "- Run the app using:\n",
    "```\n",
    "    spark-submit Unit_8_input_source_rate.py\n",
    "```   \n",
    "- Experiment with the rowsPerSecond and numPartitions options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File sink: stores the output to a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"path/to/destination/dir\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka sink: stores the output to one or more topics in Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "    .option(\"topic\", \"updates\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Console sink (for debugging): prints the output to stdout every time there is a trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Memory sink (for debugging):  stores the output in the memory of the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"mytable\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then interactively query the \"mytable\" dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ForeachBatch: runs custom write logic on every micro-batch of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Custom function that transforms and writes df to storage\n",
    "    pass\n",
    "  \n",
    "df.writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foreach sink: runs custom write logic on every row of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    # Custom function that writes row to storage\n",
    "    pass\n",
    "    \n",
    "df.writeStream \\\n",
    "    .foreach(process_row) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Complete Mode*: the entire updated result table will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Append Mode*: only the new rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Update Mode*: only the new and updated rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical structure of a Spark Streaming application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general spark streaming applications have the following structure:\n",
    "1. Read from the streaming source into a input dataframe (input source)\n",
    "2. Process the input dataframe and transform it in the output dataframe\n",
    "3. Write the output dataframe (output sink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's start with two labs to see how we can use the \"rate\" source and \"memory\" sink to work interactively:\n",
    "- Lab Unit_8_input_source_rate_output_memory.py (non interactive: submit with \"spark-submit\")\n",
    "- Lab Unit_8_interactive_streaming.ipynb (interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a basic stream app with the \"socket\" source so we can test the different output modes:\n",
    "- Lab Unit_8_structured_streaming_basics\n",
    "\n",
    "Finally let's try to implement word count in a streaming app:\n",
    "- Lab Unit_8_socket_wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Tolerance and Restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At most once: a row of data is delivered to the application at most once. It could happen that is not delivered so data could be lost.\n",
    "- At least once: a row of data is delivered to the application at least once. No data is ever lost but it could be delivered several times, so data can be duplicated. \n",
    "- Exactly once: a row of data is guaranteed to be delivered exactly once. No lost of data, no duplicated records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting in local mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit --master local[3] streaming_app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lab Structured Streaming (DStream): Review the code of a production app using the legacy API\n",
    "  - Unit_8_ssh_attack_detector-dstream_app.py\n",
    "  - Unit_8_ssh_attack_detector-submit_script.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DStream: [Spark Streaming Programming Guide (legacy)](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
