{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 9 Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "9.1. Introduction\n",
    "  9.1.1. Machine Learning Basic Concepts\n",
    "  9.1.2. Data Driven Decision Making\n",
    "  9.1.3. Supervised vs Unsupervised Learning\n",
    "  9.1.4. Data Science Life Cycle\n",
    "  \n",
    "9.2. Transformers\n",
    "\n",
    "9.3. Estimators\n",
    "\n",
    "9.4. Pipelines\n",
    "\n",
    "9.5. Evaluators\n",
    "\n",
    "9.6. Hyperparameter Tuning\n",
    "\n",
    "9.7. Data Science Lifecycle in Spark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AI vs ML vs DL](https://bigdata.cesga.es/img/spark_ml_ai_ml_dl.png)\n",
    "\n",
    "From Wikipedia:\n",
    "- Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals and humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
    "- Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. (for example Deep Learning is not statistical learning but it is part of ML)\n",
    "- Deep learning (DL) is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n",
    "\n",
    "Deep Learning is inspired in how brain works and has gained a lot of popularity in recent years, probing to be very successful in tasks like image classification, object recognition, speech recognition or language translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Driven Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Driven Decision Making](https://bigdata.cesga.es/img/spark_ml_data_driven_decision_making.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dataset**: the data that we have available about the problem we want to solve. It consists of a list of observations. It can be seen as table.\n",
    "\n",
    "- **Observation**: each row of the dataset.\n",
    "\n",
    "Each observation is divided in:\n",
    "\n",
    "- **Features**: the attributes of the observation.\n",
    "- **Label**: the output (result) of the observation (not always available, ie. unsupervised learning)\n",
    "\n",
    "The dataset is divided in:\n",
    "\n",
    "- **Training data**: the part of the observations that we use to train the model.  is a portion of the observations that train an ML algorithm to produce a model. A general practice is to split the collected data into three portions: training data, validation data, and test data. The test data portion is roughly about 70% or 80% of the original dataset.\n",
    "\n",
    "- **Test data**: the part of the observations that we use to evaluate the performance of the model.\n",
    "\n",
    "The usual split is 80% training, 20% testing.\n",
    "\n",
    "- **Validation data**: a further split of the dataset that can be used for model tuning (a.k.a. hyperparameter tuning).\n",
    "\n",
    "- **Model**: it refers to the algorithm that \"learns\" from the training data and is able to make predictions.\n",
    "- **Prediction**: the label that the model predicts for a given set of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches in machine learning: supervised learning and unsupervised learning.\n",
    "\n",
    "The main difference between both approaches is the availability of labeled data. In supervised learning we use a dataset with labeled data, but in unsupervised learning we do not have labels in our observations.\n",
    "\n",
    "Unsupervised learning is in general more difficult to do.\n",
    "\n",
    "Supervised Learning:\n",
    "- Classification: \n",
    "- Regression\n",
    "\n",
    "Unsupervised Learning:\n",
    "- Clustering\n",
    "- Collaborative Filtering\n",
    "- Dimensionality reduction (eg. PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Life Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Science Life Cycle](http://bigdata.cesga.es/img/spark_ml_datascience_lifecycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load data we use the methods we already know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = spark.read.json('/tmp/reviews_Books_5_small.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split data we use the `randomSplit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = reviews.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: Transformers, Estimators and Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overview](http://bigdata.cesga.es/img/spark_ml_overview_transformer_estimator_evaluator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Transformer is an algorithm which can transform one DataFrame into another DataFrame.\n",
    "\n",
    "Common Tranformers:\n",
    "- Binarizer\n",
    "- Bucketizer\n",
    "- VectorAssembler\n",
    "- Tokenizer\n",
    "- StopWordsRemover\n",
    "- HashingTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binarizer**: A transformer to convert numerical features to binary (0/1) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=2.5, inputCol='overall', outputCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer**: A transformer that converts the input string to lowercase and then splits it by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StopWordsRemover**: A transformer that filters out stop words from input. Note: null values from input array are preserved unless adding null to stopWords explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HashingTF**: A Transformer that converts a sequence of words into a fixed-length feature Vector. It maps a sequence of terms to their term frequencies using a hashing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer.\n",
    "\n",
    "Estimators have a `fit` method.\n",
    "\n",
    "To use an estimator first we have to use its `fit` method on some input data:\n",
    "```\n",
    "model = estimator.fit(input_data)\n",
    "```\n",
    "\n",
    "After that we will have a Transformer that we can use to generate estimations:\n",
    "```\n",
    "estimations = model.transform(data)\n",
    "```\n",
    "\n",
    "There two type of Estimators\n",
    "- The ones that are Machine Learning algorithms\n",
    "- The ones that are a sort of complex Transformers that require a previous fit step: this are known as Data Transformation Estimators\n",
    "\n",
    "For example the LogisticRegression Estimator is a classification algoritm based on the Logistic Regression algorithm, and the the MinMaxScaler is a data transformation estimator.\n",
    "\n",
    "Machine Learning Algorithms:\n",
    "- LogisticRegression\n",
    "- DecisionTreeClassifier\n",
    "- RandomForestClassifier\n",
    "- LinearRegression\n",
    "- RandomForestRegressor\n",
    "- KMeans\n",
    "- LDA\n",
    "- BisectingKMeans\n",
    "- ALS (recommendations a.k.a. collaborative filtering)\n",
    "\n",
    "Data Transformation Algorithms:\n",
    "- MixMaxScaler\n",
    "- StringIndexer\n",
    "- OneHotEncoderEstimator\n",
    "- StandardScaler\n",
    "- MaxAbsScaler\n",
    "- IDF\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression**: A classification algorithm. Supports binomial logistic regression and multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|features |label|\n",
      "+---------+-----+\n",
      "|[1.0,0.0]|0.0  |\n",
      "|[2.0,0.0]|0.0  |\n",
      "|[0.0,1.0]|1.0  |\n",
      "|[0.0,2.0]|1.0  |\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([Row(features=Vectors.dense(1.0, 0.0), label=0.0),\n",
    "                            Row(features=Vectors.dense(2.0, 0.0), label=0.0),\n",
    "                            Row(features=Vectors.dense(0.0, 1.0), label=1.0),\n",
    "                            Row(features=Vectors.dense(0.0, 2.0), label=1.0)])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+--------------------+----------+\n",
      "| features|label|       rawPrediction|         probability|prediction|\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "|[1.0,0.0]|  0.0|[2.48197057737007...|[0.92286818542131...|       0.0|\n",
      "|[2.0,0.0]|  0.0|[4.96394115474013...|[0.99306311334180...|       0.0|\n",
      "|[0.0,1.0]|  1.0|[-2.4819705773700...|[0.07713181457868...|       1.0|\n",
      "|[0.0,2.0]|  1.0|[-4.9639411547401...|[0.00693688665819...|       1.0|\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "model = lr.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|  features|label|weight|\n",
      "+----------+-----+------+\n",
      "|[20.0,0.0]|  1.0|   1.0|\n",
      "|[0.0,20.0]|  1.0|   1.0|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.createDataFrame([Row(label=1.0, weight=1.0, features=Vectors.dense(20.0, 0.0)),\n",
    "                              Row(label=1.0, weight=1.0, features=Vectors.dense(0.0, 20.0))])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+--------------------+--------------------+----------+\n",
      "|  features|label|weight|       rawPrediction|         probability|prediction|\n",
      "+----------+-----+------+--------------------+--------------------+----------+\n",
      "|[20.0,0.0]|  1.0|   1.0|[49.6394115474013...|[1.0,2.7661611663...|       0.0|\n",
      "|[0.0,20.0]|  1.0|   1.0|[-49.639411547401...|[2.76616116630809...|       1.0|\n",
      "+----------+-----+------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScaler**: Rescale each feature individually to a common range [min, max] linearly using column summary statistics, which is also known as min-max normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a dataset with the values of the temperature of a set of servers in a datacenter. For each server we have two temperatures: the inlet air temperature and the cpu temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|temperature|\n",
      "+-----------+\n",
      "|[24.0,60.0]|\n",
      "|[25.0,62.0]|\n",
      "|[25.5,63.0]|\n",
      "|[30.0,64.0]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([(Vectors.dense([24.0, 60.0]),),\n",
    "                            (Vectors.dense([25.0, 62.0]),),\n",
    "                            (Vectors.dense([25.5, 63.0]),),\n",
    "                            (Vectors.dense([30.0, 64.0]),)], [\"temperature\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol='temperature', outputCol='temperature_scaled')\n",
    "model = scaler.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit the MinMaxScaler was able to know the min and max of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([24.0, 60.0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.originalMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([30.0, 64.0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.originalMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it can act as a Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+\n",
      "|temperature|temperature_scaled       |\n",
      "+-----------+-------------------------+\n",
      "|[24.0,60.0]|[0.0,0.0]                |\n",
      "|[25.0,62.0]|[0.16666666666666666,0.5]|\n",
      "|[25.5,63.0]|[0.25,0.75]              |\n",
      "|[30.0,64.0]|[1.0,1.0]                |\n",
      "+-----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline is useful to chain multiple Transformers and Estimators together so we can create a workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[binarizer, tokenizer, remover, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluators allow us to obtain model evaluation metrics that will help us to measure how well a fitted model performs on test data.\n",
    "\n",
    "Since the evaluation metrics to calculate depend on the model we are trying to evaluate, we will have to select a evaluator that corresponds to the problme type we are solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BinaryClassificationEvaluator**: evaluates the performance of binary classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "predictions = pipeLineModel.transform(testData)\n",
    "\n",
    "aur = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MulticlassClassificationEvaluator**: evaluates the performance of multiclass classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "predictions = pipeLineModel.transform(testData)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RegressionEvaluator**: evaluates the performance of regression models as well as collaborative filtering models like ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "predictions = pipeLineModel.transform(testData)\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CrossValidator**: K-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping randomly partitioned folds which are used as separate training and test datasets e.g., with k=3 folds, K-fold cross validation will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. Each fold is used as the test set exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(hashingTF.numFeatures, [10000, 100000]) \\\n",
    "            .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "            .addGrid(lr.maxIter, [10, 20]) \\\n",
    "            .build()\n",
    "            \n",
    "cv = (CrossValidator()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(param_grid)\n",
    "      .setNumFolds(3))\n",
    "\n",
    "cv_model = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the point of view of the Spark API a CrossValidator is just another Estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overview](http://bigdata.cesga.es/img/spark_ml_crossvalidator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Life Cycle in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Science Life Cycle](http://bigdata.cesga.es/img/spark_ml_spark_lifecycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lab 1: Sentiment Analysis Amazon Books (short version)\n",
    "* Lab 2: Titanic\n",
    "* Lab 3: House prices\n",
    "* Lab 4: Movielens\n",
    "\n",
    "To help you during the labs you will find these references useful.\n",
    "\n",
    "You can find the details of the different algorithms implemented in Spark in the following references:\n",
    "- [Spark ML: Classification and regression](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "- [Spark ML: Clustering](https://spark.apache.org/docs/latest/ml-clustering.html)\n",
    "- [Spark ML: Collaborative filtering](https://spark.apache.org/docs/latest/ml-collaborative-filtering.htmll)\n",
    "- [Spark ML: Model selection and tuning](https://spark.apache.org/docs/latest/ml-tuning.html)\n",
    "\n",
    "And in case you have to deal with some legacy application implemented the old MLlib (RDD-based API) you will find useful this reference:\n",
    "- [MLlib: RDD-based API Guide](https://spark.apache.org/docs/latest/mllib-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
