{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS WITH SPARK ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML Main Concepts\n",
    "\n",
    "The Spark Machine learning API in the **spark.ml** package is based on DataFrames, there is also another Spark Machine learning API based on RDDs in the **spark.mllib** package, but as of Spark 2.0, the RDD-based API has entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API.\n",
    "\n",
    "Main concepts of Spark ML:\n",
    "\n",
    "- **Transformer**: transforms one DataFrame into another DataFrame\n",
    "\n",
    "- **Estimator**: eg. a learning algorithm that trains on a DataFrame and produces a Model\n",
    "\n",
    "- **Pipeline**: chains Transformers and Estimators to produce a Model\n",
    "\n",
    "- **Evaluator**: measures how well a fitted Model does on held-out test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon product data\n",
    "We will use a [dataset](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz)[1] that contains 8.9M book reviews from Amazon, spanning May 1996 - July 2014.\n",
    "\n",
    "Dataset characteristics:\n",
    "- Number of reviews: 8.9M\n",
    "- Size: 8.8GB (uncompressed)\n",
    "- HDFS blocks: 70 (each with 3 replicas)\n",
    "\n",
    "\n",
    "[1] Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews will be in English so we will set the locale accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANG']='en_US.UTF-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative you can set the environment before launching the notebook with:\n",
    "```bash\n",
    "export LANG=en_US.UTF-8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will use a reduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_reviews = spark.read.json('/tmp/reviews_Books_5_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the following columns:\n",
    "- `reviewText`: contains the text of the review sent by the user\n",
    "- `overall`: contains the overall rating of the product (from 1 to 5 stars)\n",
    "\n",
    "So we will only keep those columns in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_reviews = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance we will cache this dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the dataframe has now only the `reviewText` and `overall` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "We will avoid neutral reviews by keeping only reviews with 1 or 5 stars overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonneutral_reviews = all_reviews.filter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also filter out the reviews that contain no text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = nonneutral_reviews.filter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cache the new dataframe and unpersist the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.cache()\n",
    "all_reviews.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset in the training and test subsets (80% training, 20% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Pipeline\n",
    "We will now create the following pipeline:\n",
    "\n",
    "![pipeline](http://hadoop.cesga.es/files/sentiment_analysis/pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizer\n",
    "Let's generate our `label` column from the `overall` column. Our label column should contain only 0 or 1, so we have to transform the overall column so it will contain 0 for reviews below 2.5 and 1 for reviews above 2.5.\n",
    "\n",
    "For that we will use a Binarizer: A transformer to convert numerical features to binary (0/1) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Now let's start working in our feature vector. First thing is to split into words our `reviewText` column.\n",
    "\n",
    "For that we will use a Tokenizer: A transformer that converts the input string to lowercase and then splits it by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWordsRemover\n",
    "Next we have to remove all stop words (\"the\", \"and\", \"or\", etc.)\n",
    "\n",
    "Stop words are words which are filtered out (i.e. stopped) before processing of natural language data (text) because they are insignificant. For example: \"the\", \"a\", etc.\n",
    "\n",
    "For that we will use a StopWordsRemover: A transformer that filters out stop words from input. Note: null values from input array are preserved unless adding null to stopWords explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the list of words that will be removed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover.getStopWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF\n",
    "Last but not least, we will convert our sequence of words in a vector that represents the text and that we will store in our `features` column. \n",
    "\n",
    "For that we will use the HashingTF Transformer that converts a sequence of words into a fixed-length feature Vector. It maps a sequence of terms to their term frequencies using a hashing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "Now is time to choose the Estimator, ie. the ML learning algorithm to use.\n",
    "\n",
    "We are trying to predict if a review will be possitive (1.0) or negative (0.0), so this is a classification problem and we will use the LogisticRegression Estimator, one common algorithm used for classification tasks.\n",
    "\n",
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set the maximum number of iterations to 10, and the regularization param to 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will create a pipeline that merges all the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeLineModel = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well our model performs.\n",
    "\n",
    "Choose a Evaluator and get some metrics about our classification (eg. Area under ROC (AUR)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's see if we can fine tune some of the parameters, we will try the following:\n",
    "- For HashingTF we will try to tune the `numFeatures` parameter, to see how it influences our result. We will try with 10000 and 100000\n",
    "- For LinearRegression we will try to tune the `regParam` and `maxIter` parameters. For the first one we will use 0.01, 0.1 and 1.0, and for the second one we will use 10 and 20 as the maximum number of iterations.\n",
    "\n",
    "For that we will use a CrossValidator with 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(hashingTF.numFeatures, [..., ...]) \\\n",
    "            .addGrid(lr.regParam, ...) \\\n",
    "            .addGrid(...) \\\n",
    "            .build()\n",
    "            \n",
    "cv = (CrossValidator()\n",
    "      .setEstimator(...)\n",
    "      .setEvaluator(...)\n",
    "      .setEstimatorParamMaps(param_grid)\n",
    "      .setNumFolds(...))\n",
    "\n",
    "cv_model = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's evaluate the performance of the tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_predictions = cv_model.transform(testData)\n",
    "new_aur = evaluator.evaluate(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the new AUR value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_aur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
