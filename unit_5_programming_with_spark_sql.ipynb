{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5: Programming with Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "** Structured data processing **\n",
    "\n",
    "** SQL **\n",
    "\n",
    "** DataFrames **\n",
    "\n",
    "** Performance improvement **\n",
    "\n",
    "** SQLContext **\n",
    "\n",
    "** Creating DataFrames **\n",
    "\n",
    "** Saving a DataFrame **\n",
    "\n",
    "** DataFrame operations **\n",
    "\n",
    "** Query Strings **\n",
    "\n",
    "** Column Expressions **\n",
    "\n",
    "** DataFrames and RDDs **\n",
    "\n",
    "** SQL Queries **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured data processing\n",
    "Spark SQL is a Spark module for **structured data processing**.\n",
    "\n",
    "Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. **Internally, Spark SQL uses this extra information to perform extra optimizations**.\n",
    "\n",
    "**There are several ways to interact with Spark SQL including SQL, the DataFrames API and the Datasets API**. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between the various APIs based on which provides the most natural way to express a given transformation.\n",
    "\n",
    "## SQL\n",
    "One use of Spark SQL is to **execute SQL queries written using either a basic SQL syntax or HiveQL**. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the Hive Tables section. When running SQL from within another programming language the results will be returned as a DataFrame. You can also interact with the SQL interface using the command-line or over JDBC/ODBC.\n",
    "\n",
    "Reference: [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/1.6.1/sql-programming-guide.html)\n",
    "\n",
    "## DataFrames\n",
    "A DataFrame is a distributed collection of data **organized into named columns**. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "## Performance improvement\n",
    "Spark SQL and DataFrames take advantadge of the fact that they are using structured data to optimize the performance using the [Catalyst query optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html).\n",
    "\n",
    "![Performance comparison](https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png)\n",
    "Reference: [Performance improvements in Spark](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLContext\n",
    "To use Spark SQL or DataFrames we use a **SQLContext** object as the main entry point to the API, in a similar way as we use the SparkContext (sc) as the main entry point to the RDD API.\n",
    "\n",
    "There are two implementations of the SQLContext object:\n",
    "* SQLContext: basic\n",
    "* HiveContext: more advanced\n",
    "  * It is able to read Hive tables directly\n",
    "  * Supports HiveQL language\n",
    "\n",
    "In our case the Spark Shell provides us automatically with a sqlContext of the HiveContext type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.context.HiveContext"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from an existing file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an existing file in **JSON** format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processes = sqlContext.read.json('datasets/pacct_20170701_c66.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- command: string (nullable = true)\n",
      " |-- etime: double (nullable = true)\n",
      " |-- exitcode: long (nullable = true)\n",
      " |-- flag: long (nullable = true)\n",
      " |-- gid: long (nullable = true)\n",
      " |-- host: string (nullable = true)\n",
      " |-- io: long (nullable = true)\n",
      " |-- majflt: long (nullable = true)\n",
      " |-- mem: long (nullable = true)\n",
      " |-- minflt: long (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      " |-- ppid: long (nullable = true)\n",
      " |-- rw: long (nullable = true)\n",
      " |-- stime: double (nullable = true)\n",
      " |-- swaps: long (nullable = true)\n",
      " |-- tm_hour: long (nullable = true)\n",
      " |-- tm_isdst: long (nullable = true)\n",
      " |-- tm_mday: long (nullable = true)\n",
      " |-- tm_min: long (nullable = true)\n",
      " |-- tm_mon: long (nullable = true)\n",
      " |-- tm_sec: long (nullable = true)\n",
      " |-- tm_wday: long (nullable = true)\n",
      " |-- tm_yday: long (nullable = true)\n",
      " |-- tm_year: long (nullable = true)\n",
      " |-- tty: long (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      " |-- utime: double (nullable = true)\n",
      " |-- version: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an existing file in **parquet** format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processes2 = sqlContext.read.parquet('datasets/pacct_20170701.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- flag: integer (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- tty: integer (nullable = true)\n",
      " |-- exitcode: integer (nullable = true)\n",
      " |-- uid: integer (nullable = true)\n",
      " |-- gid: integer (nullable = true)\n",
      " |-- pid: integer (nullable = true)\n",
      " |-- ppid: integer (nullable = true)\n",
      " |-- tm_year: integer (nullable = true)\n",
      " |-- tm_mon: integer (nullable = true)\n",
      " |-- tm_mday: integer (nullable = true)\n",
      " |-- tm_hour: integer (nullable = true)\n",
      " |-- tm_min: integer (nullable = true)\n",
      " |-- tm_sec: integer (nullable = true)\n",
      " |-- tm_wday: integer (nullable = true)\n",
      " |-- tm_yday: integer (nullable = true)\n",
      " |-- tm_isdst: integer (nullable = true)\n",
      " |-- etime: decimal(10,2) (nullable = true)\n",
      " |-- utime: decimal(10,2) (nullable = true)\n",
      " |-- stime: decimal(10,2) (nullable = true)\n",
      " |-- mem: integer (nullable = true)\n",
      " |-- io: integer (nullable = true)\n",
      " |-- rw: integer (nullable = true)\n",
      " |-- minflt: integer (nullable = true)\n",
      " |-- majflt: integer (nullable = true)\n",
      " |-- swaps: integer (nullable = true)\n",
      " |-- command: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processes2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other formats like **CSV**, HBase, Avro, etc. are also supported using third party data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from a Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs = sqlContext.sql('select * from cesga__slurm.ft2_job_table limit 1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- job_db_inx: integer (nullable = true)\n",
      " |-- mod_time: long (nullable = true)\n",
      " |-- deleted: integer (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- array_task_str: string (nullable = true)\n",
      " |-- array_max_tasks: long (nullable = true)\n",
      " |-- array_task_pending: long (nullable = true)\n",
      " |-- cpus_req: long (nullable = true)\n",
      " |-- cpus_alloc: long (nullable = true)\n",
      " |-- derived_ec: long (nullable = true)\n",
      " |-- derived_es: string (nullable = true)\n",
      " |-- exit_code: long (nullable = true)\n",
      " |-- job_name: string (nullable = true)\n",
      " |-- id_assoc: long (nullable = true)\n",
      " |-- id_array_job: long (nullable = true)\n",
      " |-- id_array_task: long (nullable = true)\n",
      " |-- id_block: string (nullable = true)\n",
      " |-- id_job: long (nullable = true)\n",
      " |-- id_qos: long (nullable = true)\n",
      " |-- id_resv: long (nullable = true)\n",
      " |-- id_wckey: long (nullable = true)\n",
      " |-- id_user: long (nullable = true)\n",
      " |-- id_group: long (nullable = true)\n",
      " |-- kill_requid: integer (nullable = true)\n",
      " |-- mem_req: long (nullable = true)\n",
      " |-- nodelist: string (nullable = true)\n",
      " |-- nodes_alloc: long (nullable = true)\n",
      " |-- node_inx: string (nullable = true)\n",
      " |-- partition: string (nullable = true)\n",
      " |-- priority: long (nullable = true)\n",
      " |-- state: integer (nullable = true)\n",
      " |-- timelimit: long (nullable = true)\n",
      " |-- time_submit: long (nullable = true)\n",
      " |-- time_eligible: long (nullable = true)\n",
      " |-- time_start: long (nullable = true)\n",
      " |-- time_end: long (nullable = true)\n",
      " |-- time_suspended: long (nullable = true)\n",
      " |-- gres_req: string (nullable = true)\n",
      " |-- gres_alloc: string (nullable = true)\n",
      " |-- gres_used: string (nullable = true)\n",
      " |-- wckey: string (nullable = true)\n",
      " |-- track_steps: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jobs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is built from **an RDD that has a collection of Row objects** using the toDF() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "peopleRDD = sc.parallelize([('Aroa', 18, 'student'), ('Lara', 15, 'student'), ('Susana', 35, 'teacher')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peopleRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have to convert the collection of tuples in a collection of Rows and then we can transform it in an DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "peopleDF = peopleRDD.map(lambda p: Row(name=p[0], age=int(p[1]), profession=p[2])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peopleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- profession: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.write.parquet('jobs_filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates a directory in HDFS and stores there the data using one file per partition:\n",
    "```bash\n",
    "[jlopez@login6 datasets]$ hdfs dfs -ls jobs_filtered\n",
    "Found 4 items\n",
    "-rw-r--r--   3 jlopez jlopez          0 2017-07-06 13:05 jobs_filtered/_SUCCESS\n",
    "-rw-r--r--   3 jlopez jlopez       3635 2017-07-06 13:05 jobs_filtered/_common_metadata\n",
    "-rw-r--r--   3 jlopez jlopez       8801 2017-07-06 13:05 jobs_filtered/_metadata\n",
    "-rw-r--r--   3 jlopez jlopez      34596 2017-07-06 13:05 jobs_filtered/part-r-00000-e020f05d-df99-4186-ab81-26c079f8cf85.gz.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use other output formats like JSON or ORC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs.write.json('jobs_filtered_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[jlopez@login6 datasets]$ hdfs dfs -ls jobs_filtered_json\n",
    "Found 2 items\n",
    "-rw-r--r--   3 jlopez jlopez          0 2017-07-06 13:16 jobs_filtered_json/_SUCCESS\n",
    "-rw-r--r--   3 jlopez jlopez     685974 2017-07-06 13:16 jobs_filtered_json/part-r-00000-12959a3f-4bd5-46f0-8e7f-5ddc6e9ae549\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs.write.orc('jobs_filtered_orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[jlopez@login6 datasets]$ hdfs dfs -ls jobs_filtered_orc\n",
    "Found 2 items\n",
    "-rw-r--r--   3 jlopez jlopez          0 2017-07-06 13:07 jobs_filtered_orc/_SUCCESS\n",
    "-rw-r--r--   3 jlopez jlopez      28545 2017-07-06 13:07 jobs_filtered_orc/part-r-00000-3f825b83-b0e1-4b88-a55d-6cf673ed4a1d.orc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of RDDs where we had transformations and actions in this case we have:\n",
    "* Queries: **lazy** transformations that create a new DataFrame\n",
    "* Actions: trigger the execution of queries and return the data to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the first n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "|age|name|profession|\n",
      "+---+----+----------+\n",
      "| 18|Aroa|   student|\n",
      "| 15|Lara|   student|\n",
      "+---+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the first n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=18, name=u'Aroa', profession=u'student'),\n",
       " Row(age=15, name=u'Lara', profession=u'student')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=18, name=u'Aroa', profession=u'student'),\n",
       " Row(age=15, name=u'Lara', profession=u'student'),\n",
       " Row(age=35, name=u'Susana', profession=u'teacher')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/2.4.2.0-258/spark/python/pyspark/sql/context.py:239: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame([{'name': 'aroa', 'age': 17}, {'name': 'aroa', 'age': 17}, {'name': 'lara', 'age': 14}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 17|aroa|\n",
      "| 17|aroa|\n",
      "| 14|lara|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 17|aroa|\n",
      "| 14|lara|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832119"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processes_chunk = processes.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processes_chunk.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### where/filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **where** and **filter** operations are equivalent: where() is an alias for filter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "|age|name|profession|\n",
      "+---+----+----------+\n",
      "| 18|Aroa|   student|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.where('name = \"Aroa\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query is expressed using a Query String (see Query Strings section below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do projections, for example reducing the number of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pid: bigint, command: string, etime: double]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processes.select('pid', 'command', 'etime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do transformations (see Column Expressions section below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  name|(age + 100)|\n",
      "+------+-----------+\n",
      "|  Aroa|        118|\n",
      "|  Lara|        115|\n",
      "|Susana|        135|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select(peopleDF.name, peopleDF.age + 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 35|Susana|   teacher|\n",
      "| 18|  Aroa|   student|\n",
      "| 15|  Lara|   student|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.orderBy(peopleDF.age.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order is controlled by a Column Expression: .asc() and .desc() are column expressions (see the Column Expressions section below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group data (it returns a GroupedData object with additional operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f347452f8d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.groupBy('profession')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can perform operations on grouped data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the maximum/minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|profession|max(age)|\n",
      "+----------+--------+\n",
      "|   student|      18|\n",
      "|   teacher|      35|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.groupBy('profession').max('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|profession|avg(age)|\n",
      "+----------+--------+\n",
      "|   student|    16.5|\n",
      "|   teacher|    35.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.groupBy('profession').mean('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|profession|sum(age)|\n",
      "+----------+--------+\n",
      "|   student|      33|\n",
      "|   teacher|      35|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.groupBy('profession').sum('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Available GroupedData operations](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.GroupedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join DataFrames in a similar way as we did with PairRDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "professionsDF = sqlContext.createDataFrame([{'name': 'student', 'description': 'A person engaged in study'}, {'name': 'teacher', 'description': 'A person whose occupation is teaching'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+--------------------+-------+\n",
      "|age|  name|profession|         description|   name|\n",
      "+---+------+----------+--------------------+-------+\n",
      "| 18|  Aroa|   student|A person engaged ...|student|\n",
      "| 15|  Lara|   student|A person engaged ...|student|\n",
      "| 35|Susana|   teacher|A person whose oc...|teacher|\n",
      "+---+------+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.join(professionsDF, peopleDF.profession == professionsDF.name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand what type of **Query Strings** we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 18|  Aroa|   student|\n",
      "| 15|  Lara|   student|\n",
      "| 35|Susana|   teacher|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.where(peopleDF.age > 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 18|  Aroa|   student|\n",
      "| 15|  Lara|   student|\n",
      "| 35|Susana|   teacher|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.where('age > 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 18|  Aroa|   student|\n",
      "| 15|  Lara|   student|\n",
      "| 35|Susana|   teacher|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.where(peopleDF['age'] > 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some queries like select, sort, join or where can take column expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<name>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A column\n",
    "peopleDF.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can operate on columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  name|(age + 100)|\n",
      "+------+-----------+\n",
      "|  Aroa|        118|\n",
      "|  Lara|        115|\n",
      "|Susana|        135|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select(peopleDF.name, peopleDF.age + 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 35|Susana|   teacher|\n",
      "| 18|  Aroa|   student|\n",
      "| 15|  Lara|   student|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.orderBy(peopleDF.age.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 15|  Lara|   student|\n",
      "| 18|  Aroa|   student|\n",
      "| 35|Susana|   teacher|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.orderBy(peopleDF.age.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 35|Susana|   teacher|\n",
      "| 18|  Aroa|   student|\n",
      "| 15|  Lara|   student|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.orderBy(peopleDF.profession.like('stu%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to use a DataFrame as an RDD so you all the flexibility of the RDD API.\n",
    "\n",
    "It is very easy to access the underlying RDD of Rows, it is exposed under the **.rdd** property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Aroa', 4.242640687119285),\n",
       " (u'Lara', 3.872983346207417),\n",
       " (u'Susana', 5.916079783099616)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "peopleDF.rdd.map(lambda row: (row.name, math.sqrt(row.age))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "peopleDF.registerTempTable('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|age|  name|profession|\n",
      "+---+------+----------+\n",
      "| 35|Susana|   teacher|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT * FROM people WHERE age > 20').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "|age|name|profession|\n",
      "+---+----+----------+\n",
      "| 18|Aroa|   student|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('''SELECT * FROM people WHERE name LIKE \"Ar%\"''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of built-in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several builtin functions that can be useful to operate on Columns:\n",
    "\n",
    "* [List of built-in functions](http://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "When we are in doubt about how to do some transformation it is useful to check this list before proceeding to use the underlying RDD directly. For example we have methods for:\n",
    "* abs\n",
    "* avg\n",
    "* cos\n",
    "* concat\n",
    "* regexp_extract\n",
    "* regexp_replace\n",
    "* sum\n",
    "* when\n",
    "* otherwise\n",
    "* lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is no builtin function available, we can create our own user-defined functions (UDF) that will allow us to use custom python code to operate on Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "is_adult = udf(lambda n: 1 if n > 18 else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'Aroa', adult=0),\n",
       " Row(name=u'Lara', adult=0),\n",
       " Row(name=u'Susana', adult=1)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.select(peopleDF.name, is_adult(peopleDF.age).alias('adult')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "Review the documentation:\n",
    "* [pyspark.sql documentation](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html)\n",
    "\n",
    "Exercises:\n",
    "* Unit 5 Working with meteorological data, now using DataFrames. You can also try using SQL.\n",
    "* Unit 5 Sentiment Analysis: Review the Sentiment Analysis notebook that makes use of DataFrames and Spark ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
