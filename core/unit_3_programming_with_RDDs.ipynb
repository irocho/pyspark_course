{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3: Programming with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "3.1 Before we begin: Passing funtions to Spark\n",
    "3.2 Transformations\n",
    "3.3 Actions\n",
    "3.4 Loading data from HDFS\n",
    "3.5 Saving results back to HDFS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin: Passing functions to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lambda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(4))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda x: 2*x)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using normal functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd1.map(double)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is tricky to understand the scope and life cycle of variables and methods when running in a cluster. The main part of the code executes in the driver, but when parallel operations are done the functions passed are executed in the executors and data is passed around using **closures**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(4))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda x: x + 5)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plus_five(x):\n",
    "    return x + 5\n",
    "\n",
    "rdd1.map(plus_five).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1', 'a2', 'b1', 'b2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['a1', 'a2', 'b1', 'b2'])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1', 'a2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.filter(lambda x: 'a' in x)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Space: the final frontier.',\n",
       " 'These are the voyages of the starship Enterprise.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['Space: the final frontier.',\n",
    "                       'These are the voyages of the starship Enterprise.'])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Space:', 'the', 'final', 'frontier.'],\n",
       " ['These', 'are', 'the', 'voyages', 'of', 'the', 'starship', 'Enterprise.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda line: line.split())\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Space:',\n",
       " 'the',\n",
       " 'final',\n",
       " 'frontier.',\n",
       " 'These',\n",
       " 'are',\n",
       " 'the',\n",
       " 'voyages',\n",
       " 'of',\n",
       " 'the',\n",
       " 'starship',\n",
       " 'Enterprise.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd1.flatMap(lambda line: line.split())\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 1, 1, 2, 2])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.distinct()\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 1, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1, 2, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.takeSample(withReplacement=False, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 1, 1, 1, 1, 1, 2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.takeSample(withReplacement=True, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('datasets/meteogalicia.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'',\n",
       " u'',\n",
       " u'ESTACI\\ufffdN AUTOM\\ufffdTICA:Santiago-EOAS',\n",
       " u'CONCELLO:Santiago de Compostela',\n",
       " u'PROVINCIA:A Coru\\ufffda']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several files can also be loaded together at the same time but **be careful with the number of partitions generated**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile('datasets/slurmd/slurmd.log.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10) datasets/slurmd/slurmd.log.* MapPartitionsRDD[29] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |   datasets/slurmd/slurmd.log.* HadoopRDD[28] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print rdd1.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1488161034 2017 Feb 27 03:03:54 c6610 daemon info slurmd Launching batch job 467165 for UID 1053',\n",
       " u'1494997188 2017 May 17 06:59:48 c6603 user info slurmstepd task/cgroup: /slurm/uid_12329/job_706187/step_batch: alloc=16384MB mem.limit=16384MB memsw.limit=unlimited',\n",
       " u'1486762787 2017 Feb 10 22:39:47 c6604 daemon info slurmd _run_prolog: run job script took usec=39335',\n",
       " u'1492284836 2017 Apr 15 21:33:56 c6609 user info slurmstepd done with job',\n",
       " u'1489949176 2017 Mar 19 19:46:16 c6604 user info slurmstepd done with job']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.takeSample(withReplacement=False, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wholeTextFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.wholeTextFiles('datasets/slurmd/slurmd.log.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2) datasets/slurmd/slurmd.log.* MapPartitionsRDD[33] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []\\n |  WholeTextFileRDD[32] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6601',\n",
       "  u'1482336831 2016 Dec 21 17:13:51 c6601 daemon info slurmd launch task 387796.0 re'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6602',\n",
       "  u'1482485639 2016 Dec 23 10:33:59 c6602 daemon info slurmd Slurmd shutdown complet'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6603',\n",
       "  u'1482485628 2016 Dec 23 10:33:48 c6603 daemon info slurmd Slurmd shutdown complet'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6604',\n",
       "  u'1482485636 2016 Dec 23 10:33:56 c6604 daemon info slurmd Slurmd shutdown complet'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6605',\n",
       "  u'1482485640 2016 Dec 23 10:34:00 c6605 daemon info slurmd Slurmd shutdown complet'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6606',\n",
       "  u'1482485652 2016 Dec 23 10:34:12 c6606 daemon info slurmd Slurmd shutdown complet'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6607',\n",
       "  u'1482485637 2016 Dec 23 10:33:57 c6607 daemon info slurmd Slurmd shutdown complet'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6608',\n",
       "  u'1482484569 2016 Dec 23 10:16:09 c6608 daemon err slurmd error: gres/mic unable t'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6609',\n",
       "  u'1482485648 2016 Dec 23 10:34:08 c6609 daemon info slurmd Slurmd shutdown complet'),\n",
       " (u'hdfs://nameservice1/user/jlopez/datasets/slurmd/slurmd.log.c6610',\n",
       "  u'1482485646 2016 Dec 23 10:34:06 c6610 daemon info slurmd Slurmd shutdown complet')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda (filename, content): (filename, content[:80])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binaryRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from collections import namedtuple\n",
    "\n",
    "AcctRecord = namedtuple('AcctRecord',\n",
    "                        'flag version tty exitcode uid gid pid ppid '\n",
    "                        'btime etime utime stime mem io rw minflt majflt swaps '\n",
    "                        'command')\n",
    "\n",
    "def read_record(data):\n",
    "    values = struct.unpack(\"2BH6If8H16s\", data)\n",
    "    return AcctRecord(*values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AcctRecord(flag=2, version=3, tty=0, exitcode=0, uid=0, gid=0, pid=24150, ppid=24144, btime=1474162981, etime=0.0, utime=0, stime=0, mem=3924, io=0, rw=0, minflt=482, majflt=0, swaps=0, command='accton\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'),\n",
       " AcctRecord(flag=0, version=3, tty=0, exitcode=0, uid=0, gid=0, pid=24151, ppid=24144, btime=1474162981, etime=0.0, utime=0, stime=0, mem=4300, io=0, rw=0, minflt=199, majflt=0, swaps=0, command='gzip\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_rdd = sc.binaryRecords('datasets/pacct-20160919', recordLength=64)\n",
    "records = raw_rdd.map(read_record)\n",
    "records.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results back to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile('results_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will create a separate file for each partition of the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe RDDs to System Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very interesting functionality of RDDs is that you can pipe the contents of the RDD to system commands, so you can easily parallelize the execution of common tasks in multiple nodes.\n",
    "\n",
    "For each partition, all elements inside the partition are passed together (separated by newlines) as the stdin of the command, and each line of the stdout of the command will be transformed in one element of the output partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [2, 3], [4, 5], [6, 7, 8, 9]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2', u'2', u'2', u'4']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.pipe('wc -l').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.pipe('cat').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Now try to apply the above concepts to solve the following problems:\n",
    "* Unit 3 Working with meteorological data 1\n",
    "* Unit 3 Calculating Pi\n",
    "\n",
    "Optional Lab (extended):\n",
    "* Unit 3 uncompressing files in parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
