{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 7 Optimizing, Monitoring and Debugging Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "7.1. Performance considerations\n",
    "  7.1.1 RDD lineage\n",
    "  7.1.2 RDD persistance\n",
    "  7.1.3 Broadcast variables\n",
    "  7.1.4 Accumulators\n",
    "  7.1.5 Repartition and coalesce\n",
    "  \n",
    "7.2. Monitoring and Debugging\n",
    "  7.2.1. HUE/YARN UI\n",
    "  7.2.2. Spark UI and Spark History\n",
    "    7.2.2.1. Spark Event Timeline\n",
    "    7.2.2.2. Spark DAG Visualization\n",
    "    7.2.2.3. How to interprate the DAG\n",
    "  7.2.3. How to see the logs of a job\n",
    "  7.2.4. How to change the log level\n",
    "  7.2.5. Understanding how to configure memory limits\n",
    "  7.2.6. How to tune the partitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD lineage\n",
    "Each time you do a transformation in an RDD, Spark does not execute it immediately, instead it creates what is called an RDD lineage.\n",
    "\n",
    "This lineage keeps track of what are all transformations that has to be applied to produce the final RDD, from reading the data from HDFS to the different transformations that have to be applied and in which order.\n",
    "\n",
    "The lineage allows to add fault tolerance to the RDD, because in the case that something goes wrong and a executor is lost it is able to re-compute the RDD from the HDFS original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD persistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case there is an RDD that you are going to reuse it is very useful to persist it so it does not need to re-compute it each time you operate on it (by default it is persisted in memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done for a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way when you no longer need it you can unpersist it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to indicate a the storage location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "# The following is equivalent to rdd.cache()\n",
    "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "# Use disk instead of memory\n",
    "rdd.persist(StorageLevel.DISK_ONLY)\n",
    "# Use disk if it does not fit in memory (spilling)\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know more about why persistance is important and the different persistance options you can read:\n",
    "* [RDD persistance](https://spark.apache.org/docs/2.4.0/programming-guide.html#rdd-persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast variables\n",
    "If you have a **read-only** variable that must be shared between all the tasks you can do it more efficiently using a broadcast variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You create a broadcast variable in the driver\n",
    "centroidsBC = sc.broadcast([1, 2, 3])\n",
    "\n",
    "# And then you can read it in the different tasks with\n",
    "centroidsBC.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\" Source: [Spark Programming Guide](https://spark.apache.org/docs/2.4.0/programming-guide.html#broadcast-variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators are **write-only** variables (only the driver can read it) that can be used to implement counters (as in MapReduce) or sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Integer accumulator\n",
    "events = sc.accumulator(0)\n",
    "# Float accumulator\n",
    "amount = sc.accumulator(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accumulator will be incremented once per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On the executors\n",
    "events += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the driver can access the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only works in the driver\n",
    "total.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: [Spark Programming Guide](https://spark.apache.org/docs/1.6.1/programming-guide.html#accumulators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitition and Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the number of partitions of an RDD using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reduce the number of partitions, this is done more efficiently using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.coalesce(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coalesce()** is an optimized version of repartition() that allows avoiding data movement, but only if you are decreasing the number of RDD partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring and Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data WebUI\n",
    "To see the status of the cluster you can connect to the Big Data Web UI and from there you connect to HUE Web Interface.\n",
    "\n",
    "This interface will allow you to monitor your applications from a graphical interface and to access the Spark UI information.\n",
    "\n",
    "\n",
    "## Spark UI and Spark History\n",
    "From HUE by looking at the Properties tab and following the `trackingURL` link you can access the Spark UI of the running application or the Spark History server in case the application has finished.\n",
    "\n",
    "### Understanding your Apache Spark Application Through Visualization\n",
    "A Spark application is composed of:\n",
    "* jobs\n",
    "* stages\n",
    "* tasks\n",
    "\n",
    "#### Spark Event Timeline\n",
    "The timeline view is available on three levels: across all jobs, within one job, and within one stage.\n",
    "![Event Timeline](https://bigdata.cesga.es/img/spark-ui-jobs.png)\n",
    "\n",
    "We can get more details about a specific, for example Job 0:\n",
    "![Event Timeline Job](https://bigdata.cesga.es/img/spark-ui-details_for_job_0.png)\n",
    "\n",
    "And finally we can go deeper selecting a specific stage:\n",
    "![Event Timeline Stage](https://bigdata.cesga.es/img/spark-ui-details_for_stage_0.png)\n",
    "\n",
    "#### Execution DAG\n",
    "A job is associated with a chain of RDD dependencies organized in a direct acyclic graph (DAG) that we can also visualize in the Spark UI:\n",
    "\n",
    "![Execution DAG](https://bigdata.cesga.es/img/spark-ui-execution_dag.png)\n",
    "\n",
    "The greyed stage indicates that data was fetched from cache so it was not needed to re-execute that given stage: for that reason it appears as **skipped**. Whenever there is shuffling involved Spark automatically caches generated data.\n",
    "\n",
    "\n",
    "More information: [Understanding your Apache Spark Application Through Visualization](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How-to see the logs of a job\n",
    "YARN has the aggregated logs produced by the job.\n",
    "\n",
    "    yarn logs -applicationId application_1489083567361_0070 | less\n",
    "\n",
    "## Configuring the log level\n",
    "For debugging it can be useful to modify the debug level.\n",
    "\n",
    "Spark uses log4j for logging so the more versatile way to do it is changing the log4j.properties file.\n",
    "\n",
    "In some cases it can be useful to set the log level from the SparkContext:\n",
    "    sc.setLogLevel(\"INFO\")\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    \n",
    "This allows you to tune the information shown in order to debug your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding how to configure memory limits\n",
    "To increase performance Spark uses an off-heap memory through the [Project Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).\n",
    "\n",
    "![Container memory layout](http://bigdata.cesga.gal/files/spark_memory_limits.png)\n",
    "\n",
    "In case you are facing a **memoryOverhead issue**:\n",
    "* The first thing to do, is to boost ‘spark.yarn.executor.memoryOverhead’ (Tungsten: off-heap memory, recommended 10% memory)\n",
    "* The second thing to take into account, is whether your data is balanced across the partitions\n",
    "\n",
    "When using Python, decreasing the value of **spark.executor.memory** will help since Python will be all off-heap memory and would not use the RAM we reserved for heap. So, by decreasing this value, you reserve less space for the heap, thus you get more space for the off-heap operations (we want that, since Python will operate there). ‘spark.executor.memory’ is for JVM heap only.\n",
    "\n",
    "Sources and further details:\n",
    "* [Memory Overhead](https://gsamaras.wordpress.com/code/memoryoverhead-issue-in-spark/)\n",
    "* [Understanding memory management in spark for fun and profit](https://www.slideshare.net/SparkSummit/understanding-memory-management-in-spark-for-fun-and-profit)\n",
    "* [Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Spark Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging an application it can be useful to verify the values of all the Spark Properties.\n",
    "\n",
    "There are two options to do it:\n",
    "* Connecting to the Spark UI and checking the Environment tab\n",
    "* Programatically using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.yarn.jars',\n",
       "  u'local:/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/spark/jars/*,local:/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/spark/hive/*'),\n",
       " (u'spark.yarn.appMasterEnv.MKL_NUM_THREADS', u'1'),\n",
       " (u'spark.sql.queryExecutionListeners',\n",
       "  u'com.cloudera.spark.lineage.NavigatorQueryListener'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'c14-18.bd.cluster.cesga.es,c14-19.bd.cluster.cesga.es'),\n",
       " (u'spark.ui.killEnabled', u'true'),\n",
       " (u'spark.lineage.log.dir', u'/var/log/spark/lineage'),\n",
       " (u'spark.eventLog.dir', u'hdfs://nameservice1/user/spark/applicationHistory'),\n",
       " (u'spark.dynamicAllocation.executorIdleTimeout', u'60'),\n",
       " (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.io.encryption.enabled', u'false'),\n",
       " (u'spark.authenticate', u'false'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'https://c14-18.bd.cluster.cesga.es:8090/proxy/application_1560154709544_0163,https://c14-19.bd.cluster.cesga.es:8090/proxy/application_1560154709544_0163'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.executor.extraLibraryPath',\n",
       "  u'/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/hadoop/lib/native'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " (u'spark.driver.port', u'37717'),\n",
       " (u'spark.network.crypto.enabled', u'false'),\n",
       " (u'spark.yarn.historyServer.address',\n",
       "  u'http://c14-18.bd.cluster.cesga.es:18088'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.yarn.historyServer.allowTracking', u'true'),\n",
       " (u'spark.executorEnv.MKL_NUM_THREADS', u'1'),\n",
       " (u'spark.ui.enabled', u'true'),\n",
       " (u'spark.driver.appUIAddress',\n",
       "  u'http://cdh61-login2.bd.cluster.cesga.es:4041'),\n",
       " (u'spark.yarn.am.extraLibraryPath',\n",
       "  u'/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/hadoop/lib/native'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.dynamicAllocation.schedulerBacklogTimeout', u'1'),\n",
       " (u'spark.yarn.appMasterEnv.OPENBLAS_NUM_THREADS', u'1'),\n",
       " (u'spark.app.id', u'application_1560154709544_0163'),\n",
       " (u'spark.driver.host', u'cdh61-login2.bd.cluster.cesga.es'),\n",
       " (u'spark.yarn.queue', u'interactive'),\n",
       " (u'spark.app.name', u'PySparkShell'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/spark/python/lib/py4j-0.10.7-src.zip:/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/spark/python/:<CPS>/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/spark/python/lib/py4j-0.10.7-src.zip<CPS>/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/spark/python/lib/pyspark.zip'),\n",
       " (u'spark.shuffle.service.port', u'7337'),\n",
       " (u'spark.lineage.enabled', u'true'),\n",
       " (u'spark.extraListeners', u'com.cloudera.spark.lineage.NavigatorAppListener'),\n",
       " (u'spark.yarn.config.gatewayPath', u'/opt/cloudera/parcels'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.sql.warehouse.dir', u'/user/hive/warehouse'),\n",
       " (u'spark.driver.extraLibraryPath',\n",
       "  u'/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/hadoop/lib/native'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.dynamicAllocation.minExecutors', u'0'),\n",
       " (u'spark.yarn.config.replacementPath', u'{{HADOOP_COMMON_HOME}}/../../..'),\n",
       " (u'spark.dynamicAllocation.enabled', u'true'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.executorEnv.OPENBLAS_NUM_THREADS', u'1'),\n",
       " (u'spark.ui.showConsoleProgress', u'true'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS',\n",
       "  u'c14-18.bd.cluster.cesga.es:8090,c14-19.bd.cluster.cesga.es:8090')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the partitioner\n",
    "\n",
    "The partitioner is the part that decides how to split the data into the different partitions. The default is to use the HashPartitioner but in some cases you may use other partitioners in order to produce a more balanced data distribution between partitions.\n",
    "\n",
    "Apart from the HashPartitioner Spark provides the [RangePartitioner](https://spark.apache.org/docs/2.4.0/api/java/org/apache/spark/RangePartitioner.html).\n",
    "\n",
    "You can also implement your own partitioner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "* Exercise: Optimize the KMeans exercise by making use of RDD caching and broadcast variables.\n",
    "* Exercise: Explore the monitoring information for your optimized KMeans notebook, comparing it with the information for the non-optimized version, and answer the following questions:\n",
    "\n",
    "  * Explore the Jobs tab:\n",
    "    * How many jobs were run by Spark?\n",
    "    * What was the typical duration of each job? You can sort the jobs by Duration clicking in the \"Duration\" column label\n",
    "    * Explore the global event timeline\n",
    "    * Explore the job with Job Id 7:\n",
    "      * Explore the Event Timeline\n",
    "      * Explore the DAG: How many stages were run?\n",
    "      \n",
    "  * Explore the Stages tab:\n",
    "    * What was the total number of stages for all jobs?\n",
    "    * Explore the Stage 12:\n",
    "      * What was the 75th percentile duration of the tasks? \n",
    "      * What was the Input Size?\n",
    "      * Expand the Event Timeline: \n",
    "        * How was the time distributed?\n",
    "        * Compare with Stage 0: In this case the percentage of computing time is reduced, compared to the scheduler delay and task deserialization parts.\n",
    "\n",
    "  * Explore the Storage tab (notebook must be still running, it is blank for finished applications): \n",
    "    * How much data is cached?\n",
    "    * How many partitions are cached?\n",
    "    * What is the fraction of the RDD cached in memory?\n",
    "\n",
    "  * Explore the Environment tab: \n",
    "    * Was dynamic resource allocation enabled? Look at the value of the spark.dynamicAllocation.enabled property.\n",
    "    \n",
    "  * Explore the Executors tab: \n",
    "    * How many executors were used? The driver also appears in the list.\n",
    "    * In which cluster node run executor 1?\n",
    "    * Notice that, when using dynamic allocation, the executors not being used will be automatically shutdown\n",
    "    * Could we take advantadge of more executors? Check if there are executors that did not run any task.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
