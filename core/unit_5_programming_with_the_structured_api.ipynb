{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5: Programming with the Structured API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "5.1 Structured data processing\n",
    "5.2 SQL\n",
    "5.3 DataFrames\n",
    "5.4 Performance improvement\n",
    "5.5 SparkSession\n",
    "5.6 Creating DataFrames\n",
    "5.7 Saving DataFrames\n",
    "5.8 DataFrame operations\n",
    "5.9 Columns, Expressions and DataFrame operations\n",
    "5.10 Combining multiple conditions\n",
    "5.11 Using the underlying RDD\n",
    "5.12 Working with Pandas\n",
    "5.13 SQL Queries\n",
    "5.14 List of built-in functions\n",
    "5.15 UDFs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured data processing\n",
    "Spark SQL is a Spark module for **structured data processing**.\n",
    "\n",
    "Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to **perform additional optimizations**.\n",
    "\n",
    "There are several ways to interact with Spark SQL including **SQL, the DataFrames API and the Datasets API**. \n",
    "\n",
    "**When computing a result the same execution engine is used, independently of which API/language you are using to express the computation.** This unification means that developers can easily switch back and forth between the various APIs based on which provides the most natural way to express a given transformation.\n",
    "\n",
    "## SQL\n",
    "One use of Spark SQL is to **execute SQL queries written using either ANSI SQL syntax or HiveQL**. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the Hive Tables section. When running SQL from within another programming language the results will be returned as a DataFrame. You can also interact with the SQL interface using the command-line or over JDBC/ODBC.\n",
    "\n",
    "Reference: [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/2.4.0/sql-programming-guide.html)\n",
    "\n",
    "## DataFrames\n",
    "A DataFrame is a distributed collection of data **organized into named columns**. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "## Performance improvement\n",
    "Spark SQL and DataFrames take advantadge of the fact that they are using structured data to optimize the performance using the [Catalyst query optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html).\n",
    "\n",
    "![Performance comparison](https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png)\n",
    "Reference: [Performance improvements in Spark](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html)\n",
    "\n",
    "![Catalyst Optimizer](https://bigdata.cesga.es/img/catalyst_optimizer.png)\n",
    "Reference: [Spark: The Definitive Guide](https://github.com/databricks/Spark-The-Definitive-Guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "In Spark 2 to use Spark SQL or DataFrames we use a **SparkSession** object as the main entry point to the API, in a similar way as we use the SparkContext (sc) as the main entry point to the RDD API. In fact, inside the SparkSession object we have an attribute that points to the SparkContext.\n",
    "\n",
    "Just be aware, that when using previous versions of Spark, to use Spark SQL or DataFrames instead of a **SparkSession** you have to use a **SQLContext** object, and that there are two implementations of the SQLContext object:\n",
    "* SQLContext: basic\n",
    "* HiveContext: more advanced\n",
    "  * It is able to read Hive tables directly\n",
    "  * Supports HiveQL language\n",
    "\n",
    "Spark 2 unifies the SQLContext and HiveContext directly in the SparkSession object simplifying its usage.\n",
    "\n",
    "In our case the Spark Shell provides us automatically with a SparkSession object called **spark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from an existing file in JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an existing file in **JSON** format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = spark.read.json('datasets/pacct_20170701_c66.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- command: string (nullable = true)\n",
      " |-- etime: double (nullable = true)\n",
      " |-- exitcode: long (nullable = true)\n",
      " |-- flag: long (nullable = true)\n",
      " |-- gid: long (nullable = true)\n",
      " |-- host: string (nullable = true)\n",
      " |-- io: long (nullable = true)\n",
      " |-- majflt: long (nullable = true)\n",
      " |-- mem: long (nullable = true)\n",
      " |-- minflt: long (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      " |-- ppid: long (nullable = true)\n",
      " |-- rw: long (nullable = true)\n",
      " |-- stime: double (nullable = true)\n",
      " |-- swaps: long (nullable = true)\n",
      " |-- tm_hour: long (nullable = true)\n",
      " |-- tm_isdst: long (nullable = true)\n",
      " |-- tm_mday: long (nullable = true)\n",
      " |-- tm_min: long (nullable = true)\n",
      " |-- tm_mon: long (nullable = true)\n",
      " |-- tm_sec: long (nullable = true)\n",
      " |-- tm_wday: long (nullable = true)\n",
      " |-- tm_yday: long (nullable = true)\n",
      " |-- tm_year: long (nullable = true)\n",
      " |-- tty: long (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      " |-- utime: double (nullable = true)\n",
      " |-- version: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from an existing file in Parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an existing file in **parquet** format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes2 = spark.read.parquet('datasets/pacct_20170701.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- flag: integer (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- tty: integer (nullable = true)\n",
      " |-- exitcode: integer (nullable = true)\n",
      " |-- uid: integer (nullable = true)\n",
      " |-- gid: integer (nullable = true)\n",
      " |-- pid: integer (nullable = true)\n",
      " |-- ppid: integer (nullable = true)\n",
      " |-- tm_year: integer (nullable = true)\n",
      " |-- tm_mon: integer (nullable = true)\n",
      " |-- tm_mday: integer (nullable = true)\n",
      " |-- tm_hour: integer (nullable = true)\n",
      " |-- tm_min: integer (nullable = true)\n",
      " |-- tm_sec: integer (nullable = true)\n",
      " |-- tm_wday: integer (nullable = true)\n",
      " |-- tm_yday: integer (nullable = true)\n",
      " |-- tm_isdst: integer (nullable = true)\n",
      " |-- etime: decimal(10,2) (nullable = true)\n",
      " |-- utime: decimal(10,2) (nullable = true)\n",
      " |-- stime: decimal(10,2) (nullable = true)\n",
      " |-- mem: integer (nullable = true)\n",
      " |-- io: integer (nullable = true)\n",
      " |-- rw: integer (nullable = true)\n",
      " |-- minflt: integer (nullable = true)\n",
      " |-- majflt: integer (nullable = true)\n",
      " |-- swaps: integer (nullable = true)\n",
      " |-- command: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processes2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from an existing file in CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an existing file in **CSV** format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[jlopez@cdh61-login8 NYC_taxi_trip_records]$ head yellow_tripdata_2018-12.csv \n",
    "VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\n",
    "\n",
    "1,2018-12-01 00:28:22,2018-12-01 00:44:07,2,2.50,1,N,148,234,1,12,0.5,0.5,3.95,0,0.3,17.25\n",
    "1,2018-12-01 00:52:29,2018-12-01 01:11:37,3,2.30,1,N,170,144,1,13,0.5,0.5,2.85,0,0.3,17.15\n",
    "2,2018-12-01 00:12:52,2018-12-01 00:36:23,1,.00,1,N,113,193,2,2.5,0.5,0.5,0,0,0.3,3.8\n",
    "1,2018-12-01 00:35:08,2018-12-01 00:43:11,1,3.90,1,N,95,92,1,12.5,0.5,0.5,2.75,0,0.3,16.55\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = spark.read.csv('datasets/NYC_taxi_trip_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the appropriate column names we must indicate that the first line of the file contains a header with column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = spark.read.csv('datasets/NYC_taxi_trip_records', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer automatically the column data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = spark.read.csv('datasets/NYC_taxi_trip_records', header=True,\n",
    "                       inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to ensure that the date values are parsed correctly we must specify the datetime format used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = spark.read.csv('datasets/NYC_taxi_trip_records', header=True,\n",
    "                       inferSchema=True, timestampFormat=\"yyyy-MM-dd HH:mm:ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want even more control, we can provide a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DateType, IntegerType, DoubleType, DecimalType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), nullable=True),\n",
    "    StructField(\"tpep_pickup_datetime\", DateType(),\n",
    "                nullable=True,\n",
    "                metadata={\"format\": \"yyyy-MM-dd HH:mm:ss\"}),\n",
    "    StructField(\"tpep_dropoff_datetime\", DateType(),\n",
    "                nullable=True,\n",
    "                metadata={\"format\": \"yyyy-MM-dd HH:mm:ss\"}),\n",
    "    StructField(\"passenger_count\", IntegerType(), nullable=True),\n",
    "    StructField(\"trip_distance\", DoubleType(), nullable=True),\n",
    "    StructField(\"RatecodeID\", IntegerType(), nullable=True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), nullable=True),\n",
    "    StructField(\"PULocationID\", IntegerType(), nullable=True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), nullable=True),\n",
    "    StructField(\"payment_type\", IntegerType(), nullable=True),\n",
    "    StructField(\"fare_amount\", DecimalType(6,2), nullable=True),\n",
    "    StructField(\"extra\", DecimalType(6,2), nullable=True),\n",
    "    StructField(\"mta_tax\", DecimalType(6,2), nullable=True),\n",
    "    StructField(\"tip_amount\", DecimalType(6,2), nullable=True),\n",
    "    StructField(\"tolls_amount\", DecimalType(6,2), nullable=True),\n",
    "    StructField(\"improvement_surcharge\", DecimalType(6,2), nullable=True),\n",
    "    StructField(\"total_amount\", DecimalType(6,2), nullable=True)\n",
    "])\n",
    "\n",
    "trips = spark.read.csv('datasets/NYC_taxi_trip_records', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: date (nullable = true)\n",
      " |-- tpep_dropoff_datetime: date (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: decimal(6,2) (nullable = true)\n",
      " |-- extra: decimal(6,2) (nullable = true)\n",
      " |-- mta_tax: decimal(6,2) (nullable = true)\n",
      " |-- tip_amount: decimal(6,2) (nullable = true)\n",
      " |-- tolls_amount: decimal(6,2) (nullable = true)\n",
      " |-- improvement_surcharge: decimal(6,2) (nullable = true)\n",
      " |-- total_amount: decimal(6,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from an existing file in other formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other formats like Avro, HBase, etc. are also supported, in some cases using third party data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from a Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = spark.sql('select * from demos.trips limit 1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is built from **an RDD that has a collection of Row objects** using the toDF() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleRDD = sc.parallelize([('Aroa', 18, 'student'), ('Lara', 15, 'student'), ('Susana', 35, 'teacher')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peopleRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have to convert the collection of tuples in a collection of Rows and then we can transform it in an DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = peopleRDD.map(lambda p: Row(name=p[0], age=int(p[1]), profession=p[2])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peopleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- profession: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.write.parquet('trips_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates a directory in HDFS and stores there the data using one file per partition using parquet format:\n",
    "```bash\n",
    "[jlopez@cdh61-login5 ~]$ hdfs dfs -ls trips_parquet\n",
    "Found 2 items\n",
    "-rw-r--r--   3 jlopez cesga          0 2020-04-30 12:13 trips_parquet/_SUCCESS\n",
    "-rw-r--r--   3 jlopez cesga      31475 2020-04-30 12:13 trips_parquet/part-00000-b5c8ea86-7247-4ac3-b8d3-fdf477f65274-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use other output formats like JSON, CSV or Avro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.write.json('trips_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[jlopez@cdh61-login5 ~]$ hdfs dfs -ls trips_json\n",
    "Found 2 items\n",
    "-rw-r--r--   3 jlopez cesga          0 2020-04-30 12:14 trips_json/_SUCCESS\n",
    "-rw-r--r--   3 jlopez cesga     390517 2020-04-30 12:14 trips_json/part-00000-783c9be0-4925-42a8-b742-7ffa84d9053e-c000.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.write.csv('trips_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[jlopez@cdh61-login5 ~]$ hdfs dfs -ls trips_csv\n",
    "Found 2 items\n",
    "-rw-r--r--   3 jlopez cesga          0 2020-04-30 12:14 trips_csv/_SUCCESS\n",
    "-rw-r--r--   3 jlopez cesga     112517 2020-04-30 12:14 trips_csv/part-00000-15fdc03a-67ea-4127-a427-a5e525367234-c000.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.write.format('avro').save('trips_avro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[jlopez@cdh61-login5 ~]$ hdfs dfs -ls trips_avro\n",
    "Found 2 items\n",
    "-rw-r--r--   3 jlopez cesga          0 2020-04-30 12:14 trips_avro/_SUCCESS\n",
    "-rw-r--r--   3 jlopez cesga      38912 2020-04-30 12:14 trips_avro/part-00000-506db0eb-8975-4463-a20b-5e214172df53-c000.avro\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even export the dataframe back to Hive as a Hive table, for example to export it to a table in the `jlopez` database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.write.saveAsTable('jlopez.trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can query that table directly from Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of RDDs where we had transformations and actions in this case we also have them:\n",
    "* Transformations (aka Queries): **lazy** transformations that create a new DataFrame\n",
    "* Actions: trigger the execution of queries and return the data to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the first n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peopleDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the first n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations (aka Queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([Row(name='aroa', age=17), Row(name='aroa', age=17), Row(name='lara', age=14)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes_chunk = processes.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes_chunk.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### where/filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **where** and **filter** operations are equivalent: where() is an alias for filter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where('name = \"Aroa\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query is expressed using a Query String (see Query Strings section below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do projections, for example reducing the number of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processes.select('pid', 'command', 'etime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do transformations (see Column Expressions section below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peopleDF.select(col('name'), col('age') + 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can also write it using expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.select(expr('name'), expr('age + 100')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selectExpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is so common to use expressions as arguments when calling select like in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.select(expr('name'), expr('age + 100')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that there is a convenient method that allows to indicate just expressions as arguments to the select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.selectExpr('name', 'age + 100').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can easily write also aggregation queries (you can find more details on aggregations are given in the groupBy section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.selectExpr('max(age)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that would be equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "peopleDF.select(max(col('age'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.select('age').groupBy().max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### orderBy/sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **orderBy** and **sort** operations are equivalent: orderBy() is an alias for sort()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peopleDF.orderBy(col('age').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order is controlled by a Column Expression: .asc() and .desc() are column expressions (see the Column Expressions section below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group data (it returns a GroupedData object with additional operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peopleDF.groupBy('profession')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can perform operations on grouped data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the maximum/minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peopleDF.groupBy('profession').max('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.groupBy('profession').mean('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.groupBy('profession').sum('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Available GroupedData operations](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join DataFrames in a similar way as we did with PairRDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "professionsDF = spark.createDataFrame([Row(name='student', description='A person engaged in study'), Row(name='teacher', description='A person whose occupation is teaching')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(professionsDF, peopleDF.profession == professionsDF.name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding columns to an existing DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "peopleDF.withColumn('country', lit('spain')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when ... otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also express conditions using the `when ... otherwise` construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "peopleDF.withColumn('adult', when(col('age')<18, 'underage').otherwise('adult')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns, Expressions and DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some queries like select, sort, join or where can take column expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A column\n",
    "peopleDF.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to refer to the same column\n",
    "peopleDF['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just another way to refer to the same column\n",
    "col('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In other to use the `col` function you have to import it from the functions module:\n",
    "\n",
    "    from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can operate on columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.select(col('name'), col('age') + 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can give then as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.orderBy(col('age').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peopleDF.orderBy(col('age').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where(col('profession').like('stu%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, Columns are just expressions in its simplest form: `col('age')` is equivalent to `expr('age')`. But with expressions we can express more complex conditions using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "expr('age < 18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, **you can write your operations as DataFrame code or as SQL expressions (both are equivalent)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases like `select` we have seen that we have a special method called `selectExpr` that receives text as expressions. In other methods like `where` we can directly give as input expressions, dataframe column operations or text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where('age > 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where(expr('age > 10')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where(col('age') > 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where(peopleDF.age > 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where(peopleDF['age'] > 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine conditions, instead of the standard Python keywords `and`, `or` or `not` you have to use their Spark equivalents **& | ~**:\n",
    "* and -> **&**\n",
    "* or -> **|**\n",
    "* not -> **~**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way you can combine conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where((col('age') > 15) & col('profession').like('stud%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where(col('name').like('Aroa') | col('name').like('Lara')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.where(~col('profession').like('teacher')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the underlying RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to use a DataFrame as an RDD so you all the flexibility of the RDD API.\n",
    "\n",
    "It is very easy to access the underlying RDD of Rows, it is exposed under the **.rdd** property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "peopleDF.rdd.map(lambda row: (row.name, math.sqrt(row.age))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify our work, we can easily change from Spark DataFrames to Pandas DataFrames and do the operations that we need with the most appropriate technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to convert a Spark DataFrame to a Pandas DataFrame, but be careful because while the Spark DataFrame is distributed across the nodes of the Hadoop cluster, the Pandas DataFrame will be just in the node that is running the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = peopleDF.toPandas()\n",
    "\n",
    "type(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a Spark DataFrame directly from a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame({'name': ['Aroa', 'Lara', 'Susana'], 'age': [9, 6, 20]})\n",
    "\n",
    "type(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 2.4 supports both ANSI-SQL as well as Hive QL queries.\n",
    "\n",
    "For example the SELECT statement supports the following semantics:\n",
    "\n",
    "```\n",
    "SELECT [ALL|DISTINCT] named_expression[, named_expression, ...]\n",
    "    FROM relation[, relation, ...]\n",
    "    [lateral_view[, lateral_view, ...]]\n",
    "    [WHERE boolean_expression]\n",
    "    [aggregation [HAVING boolean_expression]]\n",
    "    [ORDER BY sort_expressions]\n",
    "    [CLUSTER BY expressions]\n",
    "    [DISTRIBUTE BY expressions]\n",
    "    [SORT BY sort_expressions]\n",
    "    [WINDOW named_window[, WINDOW named_window, ...]]\n",
    "    [LIMIT num_rows]\n",
    "\n",
    "named_expression:\n",
    "    : expression [AS alias]\n",
    "\n",
    "relation:\n",
    "    | join_relation\n",
    "    | (table_name|query|relation) [sample] [AS alias]\n",
    "    : VALUES (expressions)[, (expressions), ...]\n",
    "          [AS (column_name[, column_name, ...])]\n",
    "\n",
    "expressions:\n",
    "    : expression[, expression, ...]\n",
    "\n",
    "sort_expressions:\n",
    "    : expression [ASC|DESC][, expression [ASC|DESC], ...]\n",
    "```\n",
    "\n",
    "To express conditions `case...when...then...end` staments are also supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do before launching SQL queries is to **create a local temporary view of the DataFrame**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this we can start launching SQL queries against the DataFrame data just using the view name we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM people WHERE age > 20').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT * FROM people WHERE name LIKE \"Ar%\"''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this would be a more complex example where we use a `case...when...then...end` statement to create a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT name, age,\n",
    "  CASE WHEN age < 18 THEN 'No'\n",
    "       WHEN age >= 18 THEN 'Yes'\n",
    "  END AS adult\n",
    "FROM people''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL also provides a bunch of functions that you can use:\n",
    "\n",
    "```\n",
    "SHOW FUNCTIONS\n",
    "```\n",
    "These functions can be used inside SQL statements or using the DataFrames API and you can even create your own functions. More details in the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of built-in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several builtin functions that can be useful to operate on Columns:\n",
    "\n",
    "* [List of built-in functions](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "When we are in doubt about how to do some transformation it is useful to check this list before proceeding to use the underlying RDD directly. For example we have methods for:\n",
    "* abs\n",
    "* avg\n",
    "* cos\n",
    "* concat\n",
    "* regexp_extract\n",
    "* regexp_replace\n",
    "* sum\n",
    "* when\n",
    "* otherwise\n",
    "* lit\n",
    "\n",
    "NOTE: These functions are inside the `pyspark.sql.functions` module, so to use them you have to import them first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is no builtin function available, we can create our own user-defined functions (UDF) that will allow us to use custom python code to operate on Columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UDFs with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define your own UDF functions just decorating them with the `udf` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf('int')\n",
    "def is_adult(age):\n",
    "    return 1 if age >= 18 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if you prefer, you can also define them explicitely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "is_underage = udf(lambda n: 1 if n < 18 else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that you can use them to operate on columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.select(col('name'), col('age'), is_adult(col('age')).alias('adult'), is_underage(col('age')).alias('underage')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UDFs in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the functions in SQL you have to register them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register('is_adult', is_adult)\n",
    "spark.udf.register('is_underage', is_underage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can just use them as in your SQL statements as normal functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''select name, age, is_adult(age) as adult, is_underage(age) as underage from people''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [UDFs in Python](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Review the documentation:\n",
    "* [pyspark.sql documentation](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html)\n",
    "\n",
    "Exercises:\n",
    "* Unit 5 Working with meteorological data, now using DataFrames. You can also try to solve it using SQL.\n",
    "* Unit 5 Sentiment Analysis: Review the Sentiment Analysis notebook that makes use of DataFrames and Spark ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
