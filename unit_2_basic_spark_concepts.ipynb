{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2: Basic Spark Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "2.1 Spark Components\n",
    "\n",
    "2.2 RDD\n",
    "\n",
    "2.3 Partitioning\n",
    "\n",
    "2.4 Transformations vs Actions\n",
    "\n",
    "2.5 DAG\n",
    "\n",
    "2.6 Available APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Components\n",
    "\n",
    "A Spark application consists of a driver program that runs the main code of the program, distributing the operations to the rest of executors assigned by YARN to the application.\n",
    "\n",
    "![Spark Components](http://bigdata.cesga.es/tutorials/img/cluster-overview.png)\n",
    "\n",
    "Diagram taken from the [Spark Cluster Mode Overview](https://spark.apache.org/docs/2.4.0/cluster-overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further information check our [Spark Tutorial](http://bigdata.cesga.es/tutorials/spark.html#/) and the [Spark Cluster Mode Overview](https://spark.apache.org/docs/2.4.0/cluster-overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "A Resilient Distributed Dataset (RDD) is an abstraction that represents a collection of elements **distributed** across the nodes of the cluster.\n",
    "\n",
    "A RDD provides a series of methods that allow to operate with its underlying data in parallel in a very transparent way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are **resilient** because they can automatically recover in case some of the nodes fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements in a RDD are splitted between the nodes of the cluster, dividing the collection in partitions. Each partition is then processed by a given executor.\n",
    "\n",
    "![Partitioning](https://docs.google.com/drawings/d/1GAasfY7P7uaMXhvGHuZ1nOqPqv6TrE7-N96RqUn1NqE/pub?w=960&h=540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2) ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:195 []'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([1, 2, 3, 4, 5, 6], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:195 []\n"
     ]
    }
   ],
   "source": [
    "print rdd2.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, each task of an application runs against a different partition of the RDD.\n",
    "\n",
    "When using **large files** in HDFS (with many blocks) the partitions can be considered equivalent to the HDFS blocks of the given file.\n",
    "\n",
    "For **small files** (smaller than 128MB) by default spark will create two partitions, so initally only two tasks can be executed in parallel, independently of how many resources YARN has allocated to the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = sc.textFile('datasets/meteogalicia.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) datasets/meteogalicia.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  datasets/meteogalicia.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print rdd3.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = sc.textFile('datasets/meteogalicia.txt', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) datasets/meteogalicia.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  datasets/meteogalicia.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print rdd4.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations vs Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "Create a new RDD from an existing one.\n",
    "\n",
    "All transformations in Spark are **lazy**, in the sense that they do not actually do anything until an action is executed.\n",
    "\n",
    "Examples:\n",
    "* map\n",
    "* filter\n",
    "\n",
    "### Actions\n",
    "Return the result to the driver program.\n",
    "\n",
    "Examples:\n",
    "* reduce\n",
    "* collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each job is represented by a graph (specifically a [directed acyclic graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph)):\n",
    "\n",
    "![DAG](http://2.bp.blogspot.com/-5sDP78mSdlw/Ur3szYz1HpI/AAAAAAAABCo/Aak2Xn7TmnI/s1600/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available APIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are different options to use Spark in Python:\n",
    "\n",
    "* Low-Level API: Using **RDDs and PairRDDs**: the original API, low level, great flexibility\n",
    "\n",
    "* Structured API: Using **Spark SQL and DataFrames**: newer, higher level, better performance\n",
    "\n",
    "In the case of Java and Scala there is also the option of using **DataSets**: a generalization of DataFrames that allows to use typed data instead of generic Row objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Reference Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Spark RDD Programming Guide](https://spark.apache.org/docs/2.4.0/rdd-programming-guide.html)\n",
    "* [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/2.4.0/sql-programming-guide.html)\n",
    "* [Spark Python API](https://spark.apache.org/docs/2.4.0/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionaire\n",
    "Complete the [Unit 2 questionaire](https://forms.gle/knWfo8MK1A7UiiyJ7)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
