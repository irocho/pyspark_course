{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 8 Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "8.1. Introduction to Stream Processing with Spark\n",
    "  8.1.1. Spark Streaming API (DStream)\n",
    "  8.1.2. Structured Streaming API\n",
    "  8.1.3. Stream Processing Model\n",
    "  8.1.4. Streaming Sources, Sinks and Output Mode\n",
    "  8.1.5. Fault Tolerance and Restarts\n",
    "  \n",
    "8.2. Windowing and Aggregates\n",
    "  8.2.1. Stateless vs Stateful Transformations\n",
    "  8.2.2. Event Time and Windowing    \n",
    "  8.2.3. Tumbling Window\n",
    "  8.2.4. Sliding Window\n",
    "  8.2.5. Watermarking\n",
    "  \n",
    "8.3. Joins\n",
    "  8.3.1. Joining to a Static Source\n",
    "  8.3.2. Joining to Another Stream\n",
    "  8.3.3. Watermark\n",
    "  8.3.4. Outer Joins\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Stream Processing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start reviewing how Spark operates in the standard batch processing mode:\n",
    "![Standard batch processing operation](https://bigdata.cesga.es/img/spark_streaming-non_streaming_operation.png)\n",
    "In batch mode, we have a input data source, we apply some transformations and we write the output to the given storage.\n",
    "\n",
    "When procesing streaming data source we have to introduce a new axis, **time**, because in this case the input source is constantly generating new input data as time evolves.\n",
    "\n",
    "![Microbatches](https://bigdata.cesga.es/img/spark_streaming-microbatch_generation.png)\n",
    "In stream processing mode Spark divides the input data stream in micro-batches and then each micro-batch is processed in a series of small jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming API (DStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Streaming API, aka DStream, is the implementation of Spark Streaming based on RDDs. You can find it in legacy projects but for new projects the newer Structured Streaming API is recommended.\n",
    "\n",
    "NOTE: There are no longer updates to Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Structured Streaming API is the new streaming API that uses the Spark SQL engine, ie. the DataFrame API.\n",
    "\n",
    "Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "The idea behind both Spark Streaming and Structured Streaming is to divide the stream of data into **micro-batches** and each micro-batch its processed as a small job, achieving end-to-end latencies as low as 100 milliseconds.\n",
    "\n",
    "To achive lower latencies, there is also a low-latency processing mode called **Continuous Processing** which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees.\n",
    "\n",
    "The Spark SQL engine (Catalyst) takes care of running the series of jobs incrementally and continuously updating the final result as streaming data continues to arrive.\n",
    "\n",
    "The system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind spark structured streaming is to treat the live data stream as a table that is being continously appened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Socket source (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"json\") \\\n",
    "  .option(\"path\", \"path/to/source/dir\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be: parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab: Unit_8_structured_streaming-dataframe_schema.py\n",
    "- Review the code\n",
    "- Run it\n",
    "- What is the schema of the dataframe generated from the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File sink: stores the output to a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"path/to/destination/dir\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka sink: stores the output to one or more topics in Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "    .option(\"topic\", \"updates\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Console sink (for debugging): prints the output to stdout every time there is a trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ForeachBatch: runs custom write logic on every micro-batch of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Custom function that transforms and writes df to storage\n",
    "    pass\n",
    "  \n",
    "df.writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foreach sink: runs custom write logic on every row of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    # Custom function that writes row to storage\n",
    "    pass\n",
    "    \n",
    "df.writeStream \\\n",
    "    .foreach(process_row) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Complete Mode*: the entire updated result table will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Append Mode*: only the new rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Update Mode*: only the new and updated rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Unit_8_socket_wordcount:\n",
    "- Review the code of the app\n",
    "- Run the app\n",
    "- Test different output modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DStream: [Spark Streaming Programming Guide (legacy)](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
