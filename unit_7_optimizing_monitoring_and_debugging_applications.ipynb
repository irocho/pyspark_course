{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 7 Optimizing, Monitoring and Debugging Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "```\n",
    "1. Performance considerations\n",
    "1.1. RDD lineage\n",
    "1.2. RDD persistance\n",
    "1.3. Broadcast variables\n",
    "1.4. Accumulators\n",
    "1.5. Repartition and coalesce\n",
    "2. Monitoring and Debugging\n",
    "2.1. YARN RM UI\n",
    "2.2. Spark UI and Spark History\n",
    "2.2.1. Spark Event Timeline\n",
    "2.2.2. Spark DAG Visualization\n",
    "2.2.3. How to interprate the DAG\n",
    "2.3. How to see the logs of a job\n",
    "2.4. How to change the log level\n",
    "2.5. Understanding how to configure memory limits\n",
    "2.6. How to tune the partitioner\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD lineage\n",
    "Each time you do a transformation in an RDD, Spark does not execute it immediately, instead it creates what is called an RDD lineage.\n",
    "\n",
    "This lineage keeps track of what are all transformations that has to be applied to produce the final RDD, from reading the data from HDFS to the different transformations that have to be applied and in which order.\n",
    "\n",
    "The lineage allows to add fault tolerance to the RDD, because in the case that something goes wrong and a executor is lost it is able to re-compute the RDD from the HDFS original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD persistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case there is an RDD that you are going to reuse it is very useful to persist it so it does not need to re-compute it each time you operate on it (by default it is persisted in memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done for a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way when you no longer need it you can unpersist it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to indicate a the storage location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "# The following is equivalent to rdd.cache()\n",
    "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "# Use disk instead of memory\n",
    "rdd.persist(StorageLevel.DISK_ONLY)\n",
    "# Use disk if it does not fit in memory (spilling)\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know more about why persistance is important and the different persistance options you can read:\n",
    "* [RDD persistance](https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast variables\n",
    "If you have a **read-only** variable that must be shared between all the tasks you can do it more efficiently using a broadcast variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You create a broadcast variable in the driver\n",
    "centroidsBC = sc.broadcast([1, 2, 3])\n",
    "\n",
    "# And then you can read it in the different tasks with\n",
    "centroidsBC.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\" Source: [Spark Programming Guide](https://spark.apache.org/docs/1.6.1/programming-guide.html#broadcast-variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators are **write-only** variables (only the driver can read it) that can be used to implement counters (as in MapReduce) or sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Integer accumulator\n",
    "events = sc.accumulator(0)\n",
    "# Float accumulator\n",
    "amount = sc.accumulator(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accumulator will be incremented once per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On the executors\n",
    "events += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the driver can access the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only works in the driver\n",
    "total.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: [Spark Programming Guide](https://spark.apache.org/docs/1.6.1/programming-guide.html#accumulators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitition and Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the number of partitions of an RDD using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reduce the number of partitions, this is done more efficiently using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.coalesce(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coalece()** is an optimized version of repartition() that allows avoiding data movement, but only if you are decreasing the number of RDD partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN RM UI\n",
    "To see the status of the cluster you can connect to the YARN Resource Manager User Interface and see the list of running applications:\n",
    "\n",
    "* [Running Applications](http://yarn.hdp.cesga.es:8088/cluster/apps/RUNNING)\n",
    "\n",
    "## Spark UI and Spark History\n",
    "From YARN RM by following the ApplicationMaster link you can access the Spark UI of the running application or the Spark History server in case the application has finished.\n",
    "\n",
    "WARN: The VPN is needed to access the private addresses.\n",
    "\n",
    "### Understanding your Apache Spark Application Through Visualization\n",
    "A Spark application is composed of:\n",
    "* jobs\n",
    "* stages\n",
    "* tasks\n",
    "\n",
    "#### Spark Event Timeline\n",
    "The timeline view is available on three levels: across all jobs, within one job, and within one stage.\n",
    "![Event Timeline](https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.55.07-PM-1024x481.png)\n",
    "\n",
    "We can get more details about one of the jobs:\n",
    "![Event Timeline Job](https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.56.30-PM-1024x426.png)\n",
    "\n",
    "And finally for a stage:\n",
    "![Event Timeline Stage](https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.57.36-PM-1024x823.png)\n",
    "\n",
    "#### Execution DAG\n",
    "A job is associated with a chain of RDD dependencies organized in a direct acyclic graph (DAG) that we can also visualize in the Spark UI:\n",
    "\n",
    "![Execution DAG](https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.00.59-PM.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Source: [Understanding your Apache Spark Application Through Visualization](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How-to see the logs of a job\n",
    "YARN has the aggregated logs produced by the job.\n",
    "\n",
    "    yarn logs -applicationId application_1489083567361_0070 | less\n",
    "\n",
    "## Configuring the log level\n",
    "For debugging it can be useful to modify the debug level.\n",
    "\n",
    "Spark uses log4j for logging so the more versatile way to do it is changing the log4j.properties file.\n",
    "\n",
    "In some cases it can be useful to set the log level from the SparkContext:\n",
    "    sc.setLogLevel(\"INFO\")\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    \n",
    "This allows you to tune the information shown in order to debug your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding how to configure memory limits\n",
    "To increase performance Spark uses an off-heap memory through the [Project Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).\n",
    "\n",
    "![Container memory layout](http://bigdata.cesga.gal/files/spark_memory_limits.png)\n",
    "\n",
    "In case you are facing a **memoryOverhead issue**:\n",
    "* The first thing to do, is to boost ‘spark.yarn.executor.memoryOverhead’ (Tungsten: off-heap memory, recommended 10% memory)\n",
    "* The second thing to take into account, is whether your data is balanced across the partitions\n",
    "\n",
    "When using Python, decreasing the value of **spark.executor.memory** will help since Python will be all off-heap memory and would not use the RAM we reserved for heap. So, by decreasing this value, you reserve less space for the heap, thus you get more space for the off-heap operations (we want that, since Python will operate there). ‘spark.executor.memory’ is for JVM heap only.\n",
    "\n",
    "Sources and further details:\n",
    "* [Memory Overhead](https://gsamaras.wordpress.com/code/memoryoverhead-issue-in-spark/)\n",
    "* [Understanding memory management in spark for fun and profit](https://www.slideshare.net/SparkSummit/understanding-memory-management-in-spark-for-fun-and-profit)\n",
    "* [Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Spark Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging an application it can be useful to verify the values of all the Spark Properties.\n",
    "\n",
    "There are two options to do it:\n",
    "* Connecting to the Spark UI and checking the Environment tab\n",
    "* Programatically using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.extraLibraryPath',\n",
       "  u'/opt/intel/mkl/lib/intel64/:/opt/intel/lib/intel64/'),\n",
       " (u'spark.history.kerberos.keytab', u'none'),\n",
       " (u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.driver.extraClassPath',\n",
       "  u'/mnt/EMC/fs-hadoop/netlib-dependencies/target/netlib-dependencies-0.0.1-SNAPSHOT-jar-with-dependencies.jar'),\n",
       " (u'spark.yarn.scheduler.heartbeat.interval-ms', u'5000'),\n",
       " (u'spark.history.ui.port', u'18080'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.history.fs.logDirectory', u'hdfs:///spark-history'),\n",
       " (u'spark.master', u'yarn-client'),\n",
       " (u'spark.yarn.containerLauncherMaxThreads', u'25'),\n",
       " (u'spark.yarn.historyServer.address', u'c13-18.node.int.cesga.es:18080'),\n",
       " (u'spark.yarn.queue', u'interactive'),\n",
       " (u'spark.app.name', u'PySparkShell'),\n",
       " (u'spark.eventLog.dir', u'hdfs:///spark-history'),\n",
       " (u'spark.yarn.preserve.staging.files', u'false'),\n",
       " (u'spark.yarn.submit.file.replication', u'3'),\n",
       " (u'spark.history.kerberos.principal', u'none'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.history.provider',\n",
       "  u'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.yarn.max.executor.failures', u'3'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.dynamicAllocation.enabled', u'true'),\n",
       " (u'spark.executor.extraClassPath',\n",
       "  u'/mnt/EMC/fs-hadoop/netlib-dependencies/target/netlib-dependencies-0.0.1-SNAPSHOT-jar-with-dependencies.jar'),\n",
       " (u'spark.driver.extraLibraryPath',\n",
       "  u'/opt/intel/mkl/lib/intel64/:/opt/intel/lib/intel64/')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the partitioner\n",
    "\n",
    "The partitioner is the part that decides how to split the data into the different partitions. The default is to use the HashPartitioner but in some cases you may use other partitioners in order to produce a more balanced data distribution between partitions.\n",
    "\n",
    "Apart from the HashPartitioner Spark provides the [RangePartitioner](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/RangePartitioner.html).\n",
    "\n",
    "You can also implement your own partitioner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "* Exercise: Optimize the KMeans exercise by making use of RDD caching and broadcast variables.\n",
    "* Exercise: Explore the monitoring information from application_1498464222862_3294 and answer the following questions:\n",
    "  * How many jobs were run by Spark? 846\n",
    "  * Explore the executors tab: How many executors were used? 54 + driver\n",
    "  * Explore the Storage tab: How much data was cached? 3.7GB What was the fraction of the RDD cached in memory? 100% How many partitions? 71\n",
    "  * Explore the Environment tab: What was the executor.memoryOverhead value? 384MB\n",
    "  * In the jobs tab, explore the global event timeline\n",
    "  * What was the typical duration of each job? Most less than 1 second and some around 10 seconds.\n",
    "  * Look into Job 569 and explore its DAG visualization. How many stages formed the Job? 2\n",
    "  * Inside this job look into Stage 1215 and open the Event Timelime:\n",
    "    * What was the dominant time in each task of this stage? Computing time.\n",
    "    * How many task were executed by each executor? 1 or 2.\n",
    "    * Could we take advantadge of more executors? Yes.\n",
    "    * In the case of the second stage of this job (Stage 1216) how was the time distributed? In this case the computing time is reduced and we have scheduler delay time, task deserialization time, shuffle read and result serialization time taking an important amount of the total time of each task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
