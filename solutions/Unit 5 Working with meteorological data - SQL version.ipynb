{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with meteorological data using DataFrames (SQL version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use meteorolical data from Meteogalicia that contains the measurements of a weather station in Santiago during June 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('datasets/meteogalicia.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def parse_row(line):\n",
    "    \"\"\"Convert a line into a Row\"\"\"\n",
    "    # All data lines start with 6 spaces\n",
    "    if line.startswith('      '):\n",
    "        codigo = int(line[:17].strip())\n",
    "        datahora = line[17:40]\n",
    "        data, hora = datahora.split()\n",
    "        parametro = line[40:82].strip()\n",
    "        valor = float(line[82:].replace(',', '.'))\n",
    "        return [Row(codigo=codigo, data=data, hora=hora, parametro=parametro, valor=valor)]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using flatMap we have the flexibility to return nothing from a call to the function, this is accomplished returning and empty array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rdd.flatMap(parse_row).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Temporary View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch SQL queries we have first to create a temporary view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|   codigo|   bigint|   null|\n",
      "|     data|   string|   null|\n",
      "|     hora|   string|   null|\n",
      "|parametro|   string|   null|\n",
      "|    valor|   double|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe data').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   16704|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data.count()\n",
    "spark.sql('select count(*) from data').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = data.where(data.parametro.like('Temperatura media %'))\n",
    "t = spark.sql('select * from data where parametro like \"Temperatura media %\"')\n",
    "t.createOrReplaceTempView('t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the maximum temperature of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(valor)|\n",
      "+----------+\n",
      "|      34.4|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t.groupBy().max('valor').show()\n",
    "spark.sql('select max(valor) from t').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the minimum temperature of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(valor)|\n",
      "+----------+\n",
      "|   -9999.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t.groupBy().min('valor').show()\n",
    "spark.sql('select min(valor) from t').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value -9999 is a code used to indicate a non registered value (N/A).\n",
    "\n",
    "If we look to the possible values of \"Códigos de validación\" we see valid points have the code 1, so we can concentrate our efforts on data with code 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(valor)|\n",
      "+----------+\n",
      "|      9.09|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t.where(t.codigo == 1).groupBy().min('valor').show()\n",
    "spark.sql('select min(valor) from t where codigo=1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the average temperature per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      data|        avg(valor)|\n",
      "+----------+------------------+\n",
      "|2017-06-22| 19.56493055555555|\n",
      "|2017-06-07| 17.76305555555556|\n",
      "|2017-06-24|           17.6775|\n",
      "|2017-06-29|13.477083333333331|\n",
      "|2017-06-19|25.422708333333333|\n",
      "|2017-06-03|14.511736111111105|\n",
      "|2017-06-23| 18.57861111111111|\n",
      "|2017-06-28|15.242361111111105|\n",
      "|2017-06-12|20.020138888888884|\n",
      "|2017-06-30|             11.59|\n",
      "|2017-06-26|18.298125000000002|\n",
      "|2017-06-04|14.889375000000005|\n",
      "|2017-06-18|26.350069444444443|\n",
      "|2017-06-06|14.901041666666666|\n",
      "|2017-06-09| 17.86694444444445|\n",
      "|2017-06-21| 23.28430555555555|\n",
      "|2017-06-25| 19.57138888888889|\n",
      "|2017-06-14| -51.6271527777778|\n",
      "|2017-06-16|22.042708333333337|\n",
      "|2017-06-11|17.806250000000006|\n",
      "|2017-06-08| 17.49979166666667|\n",
      "|2017-06-13|18.769027777777776|\n",
      "|2017-06-01|17.179580419580425|\n",
      "|2017-06-02|16.007500000000004|\n",
      "|2017-06-27|17.025555555555556|\n",
      "|2017-06-17|25.475902777777772|\n",
      "|2017-06-15|18.135486111111103|\n",
      "|2017-06-20|26.977916666666665|\n",
      "|2017-06-05| 13.67486111111111|\n",
      "|2017-06-10|19.207222222222224|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t.groupBy(t.data).mean('valor').show(30)\n",
    "spark.sql('select data, mean(valor) from t group by data').show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the results sorted by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      data|        avg(valor)|\n",
      "+----------+------------------+\n",
      "|2017-06-01|17.179580419580425|\n",
      "|2017-06-02|16.007500000000004|\n",
      "|2017-06-03|14.511736111111105|\n",
      "|2017-06-04|14.889375000000005|\n",
      "|2017-06-05| 13.67486111111111|\n",
      "|2017-06-06|14.901041666666666|\n",
      "|2017-06-07| 17.76305555555556|\n",
      "|2017-06-08| 17.49979166666667|\n",
      "|2017-06-09| 17.86694444444445|\n",
      "|2017-06-10|19.207222222222224|\n",
      "|2017-06-11|17.806250000000006|\n",
      "|2017-06-12|20.020138888888884|\n",
      "|2017-06-13|18.769027777777776|\n",
      "|2017-06-14| -51.6271527777778|\n",
      "|2017-06-15|18.135486111111103|\n",
      "|2017-06-16|22.042708333333337|\n",
      "|2017-06-17|25.475902777777772|\n",
      "|2017-06-18|26.350069444444443|\n",
      "|2017-06-19|25.422708333333333|\n",
      "|2017-06-20|26.977916666666665|\n",
      "|2017-06-21| 23.28430555555555|\n",
      "|2017-06-22| 19.56493055555555|\n",
      "|2017-06-23| 18.57861111111111|\n",
      "|2017-06-24|           17.6775|\n",
      "|2017-06-25| 19.57138888888889|\n",
      "|2017-06-26|18.298125000000002|\n",
      "|2017-06-27|17.025555555555556|\n",
      "|2017-06-28|15.242361111111105|\n",
      "|2017-06-29|13.477083333333331|\n",
      "|2017-06-30|             11.59|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t.groupBy(t.data).mean('valor').sort('data').show(30)\n",
    "spark.sql('select data, mean(valor) from t group by data order by data').show(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
